{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 1\n",
    "import re\n",
    "\n",
    "def generate_ngrams(s,n):\n",
    "    #Convert to the lowercase\n",
    "    s= s.lower()\n",
    "    \n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]',' ',s)\n",
    "    \n",
    "    #Break sentence in the toekn,remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    #Using zip function to help us generate n-grams \n",
    "    #Concatenate the tokens into ngrams and return \n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['when performing',\n",
       " 'performing machine',\n",
       " 'machine learning',\n",
       " 'learning tasks',\n",
       " 'tasks related',\n",
       " 'related to',\n",
       " 'to natural',\n",
       " 'natural language',\n",
       " 'language processing',\n",
       " 'processing we',\n",
       " 'we usually',\n",
       " 'usually need',\n",
       " 'need to',\n",
       " 'to generate',\n",
       " 'generate n',\n",
       " 'n grams',\n",
       " 'grams from',\n",
       " 'from input',\n",
       " 'input sentences',\n",
       " 'sentences for',\n",
       " 'for example',\n",
       " 'example in',\n",
       " 'in text',\n",
       " 'text classification',\n",
       " 'classification tasks',\n",
       " 'tasks in',\n",
       " 'in addition',\n",
       " 'addition to',\n",
       " 'to using',\n",
       " 'using each',\n",
       " 'each individual',\n",
       " 'individual token',\n",
       " 'token found',\n",
       " 'found in',\n",
       " 'in the',\n",
       " 'the corpus',\n",
       " 'corpus we',\n",
       " 'we may',\n",
       " 'may want',\n",
       " 'want to',\n",
       " 'to add',\n",
       " 'add bi',\n",
       " 'bi grams',\n",
       " 'grams or',\n",
       " 'or tri',\n",
       " 'tri grams',\n",
       " 'grams as',\n",
       " 'as features',\n",
       " 'features to',\n",
       " 'to represent',\n",
       " 'represent our',\n",
       " 'our documents']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "s = \"When performing machine learning tasks related to natural language processing, we usually need to generate n-grams from input sentences. For example, in text classification tasks, in addition to using each individual token found in the corpus, we may want to add bi-grams or tri-grams as features to represent our documents\"\n",
    "generate_ngrams(s, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('when', 'performing', 'machine'), ('performing', 'machine', 'learning'), ('machine', 'learning', 'tasks'), ('learning', 'tasks', 'related'), ('tasks', 'related', 'to'), ('related', 'to', 'natural'), ('to', 'natural', 'language'), ('natural', 'language', 'processing'), ('language', 'processing', 'we'), ('processing', 'we', 'usually'), ('we', 'usually', 'need'), ('usually', 'need', 'to'), ('need', 'to', 'generate'), ('to', 'generate', 'n'), ('generate', 'n', 'grams'), ('n', 'grams', 'from'), ('grams', 'from', 'input'), ('from', 'input', 'sentences'), ('input', 'sentences', 'for'), ('sentences', 'for', 'example'), ('for', 'example', 'in'), ('example', 'in', 'text'), ('in', 'text', 'classification'), ('text', 'classification', 'tasks'), ('classification', 'tasks', 'in'), ('tasks', 'in', 'addition'), ('in', 'addition', 'to'), ('addition', 'to', 'using'), ('to', 'using', 'each'), ('using', 'each', 'individual'), ('each', 'individual', 'token'), ('individual', 'token', 'found'), ('token', 'found', 'in'), ('found', 'in', 'the'), ('in', 'the', 'corpus'), ('the', 'corpus', 'we'), ('corpus', 'we', 'may'), ('we', 'may', 'want'), ('may', 'want', 'to'), ('want', 'to', 'add'), ('to', 'add', 'bi'), ('add', 'bi', 'grams'), ('bi', 'grams', 'or'), ('grams', 'or', 'tri'), ('or', 'tri', 'grams'), ('tri', 'grams', 'as'), ('grams', 'as', 'features'), ('as', 'features', 'to'), ('features', 'to', 'represent'), ('to', 'represent', 'our'), ('represent', 'our', 'documents')]\n"
     ]
    }
   ],
   "source": [
    "#PART 1 testing using NLTK\n",
    "import re\n",
    "from nltk.util import ngrams\n",
    "\n",
    "s = s.lower()\n",
    "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "output = list(ngrams(tokens, 3))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ngram' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d15a790bed6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw1_w2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ngram' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(model['token','found'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cats',\n",
       " 'chase',\n",
       " 'rats',\n",
       " 'cats',\n",
       " 'meow',\n",
       " 'rats',\n",
       " 'chatter',\n",
       " 'cats',\n",
       " 'chase',\n",
       " 'birds']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"cats chase rats cats meow rats chatter cats chase birds\"\n",
    "model=generate_ngrams(sentence,1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from collections import Counter,defaultdict\n",
    "model = defaultdict(lambda:defaultdict(lambda:0))\n",
    "snew = \"\"\n",
    "for string in reuters.sents():\n",
    "    snew += \" \".join(string)\n",
    "\n",
    "ngram=generate_ngrams(snew,n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ngram)):\n",
    "    [w1, w2, w3] = ngram[i].split()\n",
    "    model[(w1, w2)][w3] += 1\n",
    "\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] = round((model[w1_w2][w3] / total_count),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company': 0.16,\n",
       " '0': 0.01,\n",
       " 'pilots': 0.01,\n",
       " 'iranian': 0.01,\n",
       " 'public': 0.01,\n",
       " 'bank': 0.06,\n",
       " 'european': 0.01,\n",
       " 'trade': 0.01,\n",
       " 'treasury': 0.01,\n",
       " 'price': 0.02,\n",
       " 'central': 0.05,\n",
       " 'mine': 0.01,\n",
       " 'decrease': 0.03,\n",
       " 'fed': 0.01,\n",
       " 'emirate': 0.01,\n",
       " 'tariff': 0.01,\n",
       " 'energy': 0.01,\n",
       " 'overseas': 0.01,\n",
       " 'office': 0.01,\n",
       " 'new': 0.03,\n",
       " 'plant': 0.01,\n",
       " 'newspaper': 0.01,\n",
       " 'officials': 0.01,\n",
       " 'dealers': 0.01,\n",
       " 'last': 0.01,\n",
       " 'turkish': 0.01,\n",
       " 'same': 0.01,\n",
       " 'increase': 0.1,\n",
       " 'commerzbank': 0.01,\n",
       " 'bulk': 0.01,\n",
       " 'traders': 0.01,\n",
       " 'csce': 0.02,\n",
       " 'exchange': 0.01,\n",
       " 'supreme': 0.01,\n",
       " 'buena': 0.01,\n",
       " 'move': 0.02,\n",
       " 'benchmark': 0.01,\n",
       " 'sale': 0.01,\n",
       " 'transaction': 0.01,\n",
       " 'house': 0.01,\n",
       " 'options': 0.01,\n",
       " 'government': 0.01,\n",
       " 'higher': 0.01,\n",
       " 'official': 0.01,\n",
       " 'pound': 0.01,\n",
       " 'storm': 0.01,\n",
       " 'canadian': 0.01,\n",
       " 'average': 0.01,\n",
       " 'court': 0.01,\n",
       " 'usda': 0.01,\n",
       " 'redstone': 0.01,\n",
       " 'italian': 0.01,\n",
       " 'agriculture': 0.01,\n",
       " 'focus': 0.01,\n",
       " 'economic': 0.01,\n",
       " 'china': 0.01,\n",
       " 'planes': 0.01,\n",
       " 'third': 0.01,\n",
       " 'commission': 0.01,\n",
       " 'auditors': 0.01,\n",
       " 'tax': 0.01,\n",
       " 'u': 0.01,\n",
       " 'world': 0.01,\n",
       " 'time': 0.01,\n",
       " 'seamen': 0.01,\n",
       " 'federal': 0.01}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(model['today','the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing MLE\n",
    "def MLE(sp_word,model):\n",
    "    count=0\n",
    "    for word in model:\n",
    "        if(word==sp_word):\n",
    "               count+=1\n",
    "    N= len(model)\n",
    "    return count/N\n",
    "#Test1\n",
    "MLE(\"cats\",model)\n",
    "#Test2\n",
    "MLE(\"rat\",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0625"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Adding the laplace smoothing\n",
    "#Testing MLE\n",
    "def MLElap(sp_word,model):\n",
    "    count=0\n",
    "    unique_words=set(model)\n",
    "    \n",
    "    for word in model:\n",
    "       \n",
    "        if(word==sp_word):\n",
    "               count+=1\n",
    "    V = len(unique_words) \n",
    "    N = len(model)\n",
    "    \n",
    "    return count+1/(N+V)\n",
    "MLElap(\"rat\",model)\n",
    "#MLE(\"rat\",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
